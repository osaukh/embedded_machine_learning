{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "wRuwL",
      "launcher_item_id": "NI888"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hQt-Hrbbj2h"
      },
      "source": [
        "# Classification with One Layer MLP\n",
        "\n",
        "In this exercise, you will build a neural network with one hidden layer.\n",
        "\n",
        "**Goals:**\n",
        "- Implement a 2-class classification neural network with a single hidden layer\n",
        "- Use units with a non-linear activation function\n",
        "- Compute the cross entropy loss\n",
        "- Implement forward and backward propagation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqL19WWBbj2j"
      },
      "source": [
        "Let's first import all the packages that you will need during this assignment.\n",
        "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
        "- [sklearn](http://scikit-learn.org/stable/) provides simple and efficient tools for data mining and data analysis.\n",
        "- [matplotlib](http://matplotlib.org) is a library for plotting graphs in Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzWtm8tXbj2j"
      },
      "source": [
        "# Package imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import sklearn.datasets\n",
        "import sklearn.linear_model\n",
        "\n",
        "np.random.seed(1) # set a seed so that the results are consistent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjTaqw5kbj2k"
      },
      "source": [
        "## 1 - Dataset ##\n",
        "\n",
        "The following code will generate a \"flower\" 2-class dataset into variables `X` and `Y`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBudWpazbj2k"
      },
      "source": [
        "def load_planar_dataset():\n",
        "    np.random.seed(1)\n",
        "    m = 400 # number of examples\n",
        "    N = int(m/2) # number of points per class\n",
        "    D = 2 # dimensionality\n",
        "    X = np.zeros((m,D)) # data matrix where each row is a single example\n",
        "    Y = np.zeros((m,1), dtype='uint8') # labels vector (0 for red, 1 for blue)\n",
        "    a = 4 # maximum ray of the flower\n",
        "\n",
        "    for j in range(2):\n",
        "        ix = range(N*j,N*(j+1))\n",
        "        t = np.linspace(j*3.12,(j+1)*3.12,N) + np.random.randn(N)*0.2 # theta\n",
        "        r = a*np.sin(4*t) + np.random.randn(N)*0.2 # radius\n",
        "        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
        "        Y[ix] = j\n",
        "\n",
        "    X = X.T\n",
        "    Y = Y.T\n",
        "\n",
        "    return X, Y\n",
        "\n",
        "X, Y = load_planar_dataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PIP7p9dbj2l"
      },
      "source": [
        "Visualize the dataset using matplotlib. The data looks like a \"flower\" with some red (label y=0) and some blue (y=1) points. Your goal is to build a model to fit this data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pW-1aT7gbj2l"
      },
      "source": [
        "# Visualize the data:\n",
        "plt.scatter(X[0, :], X[1, :], c=Y, s=40, cmap=plt.cm.Spectral, edgecolors='k');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZV9Vl-ubj2m"
      },
      "source": [
        "You have:\n",
        "* a numpy-array (matrix) X that contains your features (x1, x2)\n",
        "* a numpy-array (vector) Y that contains your labels (red:0 or blue:1).\n",
        "\n",
        "Lets first get a better sense of what our data is like.\n",
        "\n",
        "**Exercise**: How many training examples do you have? In addition, what is the `shape` of the variables `X` and `Y`?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CtWIsbJbj2m"
      },
      "source": [
        "### START CODE HERE ###\n",
        "shape_X = None\n",
        "shape_Y = None\n",
        "m = None  # training set size\n",
        "### END CODE HERE ###\n",
        "\n",
        "print ('The shape of X is: ' + str(shape_X))\n",
        "print ('The shape of Y is: ' + str(shape_Y))\n",
        "print ('I have m = %d training examples!' % (m))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTkxNbFXbj2n"
      },
      "source": [
        "## 2 - Simple Logistic Regression\n",
        "\n",
        "Before building a full neural network, lets first see how logistic regression performs on this problem. You can use sklearn's built-in functions to do that. Run the code below to train a logistic regression classifier on the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rth20ND6bj2n"
      },
      "source": [
        "# Train the logistic regression classifier\n",
        "clf = sklearn.linear_model.LogisticRegressionCV();\n",
        "clf.fit(X.T, Y.T);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41odQWgCbj2n"
      },
      "source": [
        "You can now plot the decision boundary of these models. Run the code below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "qfzaS3Twbj2o"
      },
      "source": [
        "def plot_decision_boundary(model, X, y):\n",
        "    # Set min and max values and give it some padding\n",
        "    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n",
        "    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n",
        "    h = 0.01\n",
        "    # Generate a grid of points with distance h between them\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "    # Predict the function value for the whole grid\n",
        "    Z = model(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    # Plot the contour and training examples\n",
        "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
        "    plt.ylabel('x2')\n",
        "    plt.xlabel('x1')\n",
        "    plt.scatter(X[0, :], X[1, :], c=y, cmap=plt.cm.Spectral, edgecolors='k')\n",
        "\n",
        "# Plot the decision boundary for logistic regression\n",
        "plot_decision_boundary(lambda x: clf.predict(x), X, Y)\n",
        "plt.title(\"Logistic Regression\")\n",
        "\n",
        "# Print accuracy\n",
        "LR_predictions = clf.predict(X.T)\n",
        "print ('Accuracy of logistic regression: %d ' % float((np.dot(Y,LR_predictions) + np.dot(1-Y,1-LR_predictions))/float(Y.size)*100) +\n",
        "       '% ' + \"(percentage of correctly labelled datapoints)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wq_4AVxbj2p"
      },
      "source": [
        "The dataset is not linearly separable, so logistic regression doesn't perform well. Hopefully a neural network will do better. Let's try this now!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqUTwy3obj2p"
      },
      "source": [
        "## 3 - Neural Network model\n",
        "\n",
        "Now we are going to train a Neural Network with a single hidden layer.\n",
        "\n",
        "**Here is our model**:\n",
        "<img src=\"https://github.com/gemaatienza/Deep-Learning-Coursera/raw/691374a31a373841bd3cad8cfa96fbef0b04a3dc/1.%20Neural%20Networks%20and%20Deep%20Learning/images/classification_kiank.png\">\n",
        "\n",
        "**Mathematically**:\n",
        "\n",
        "For one example $x^{(i)}$:\n",
        "$$z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1] (i)}\\tag{1}$$\n",
        "$$a^{[1] (i)} = \\tanh(z^{[1] (i)})\\tag{2}$$\n",
        "$$z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2] (i)}\\tag{3}$$\n",
        "$$\\hat{y}^{(i)} = a^{[2] (i)} = \\sigma(z^{ [2] (i)})\\tag{4}$$\n",
        "$$y^{(i)}_{prediction} = \\begin{cases} 1 & \\mbox{if } a^{[2](i)} > 0.5 \\\\ 0 & \\mbox{otherwise } \\end{cases}\\tag{5}$$\n",
        "\n",
        "Given the predictions on all the examples, you can also compute the cost $J$ (cross-entropy loss) as follows:\n",
        "$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small \\tag{6}$$\n",
        "\n",
        "**The general methodology to build a Neural Network is to**:\n",
        "* Define the neural network structure ( # of input units,  # of hidden units, etc).\n",
        "* Initialize the model's parameters\n",
        "* Loop:\n",
        "  - Implement forward propagation\n",
        "  - Compute loss\n",
        "  - Implement backward propagation to get the gradients\n",
        "  - Update parameters (gradient descent)\n",
        "\n",
        "You often build helper functions to compute steps 1-3 and then merge them into one function we call `nn_model()`. Once you've built `nn_model()` and learnt the right parameters, you can make predictions on new data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tGZi4Nybj2p"
      },
      "source": [
        "### 3.1 - Defining the neural network structure ####\n",
        "\n",
        "**Exercise**: Define three variables:\n",
        "* n_x: the size of the input layer\n",
        "* n_h: the size of the hidden layer (set this to 4)\n",
        "* n_y: the size of the output layer\n",
        "\n",
        "Use shapes of X and Y to find n_x and n_y. Also, hard code the hidden layer size to be 4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "3OnnbM4Hbj2p"
      },
      "source": [
        "def layer_sizes(X, Y):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    X -- input dataset of shape (input size, number of examples)\n",
        "    Y -- labels of shape (output size, number of examples)\n",
        "    Returns:\n",
        "    n_x -- the size of the input layer\n",
        "    n_h -- the size of the hidden layer\n",
        "    n_y -- the size of the output layer\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ###\n",
        "    n_x = None # size of input layer\n",
        "    n_h = 4\n",
        "    n_y = None # size of output layer\n",
        "    ### END CODE HERE ###\n",
        "    return (n_x, n_h, n_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2EXudZXbj2q"
      },
      "source": [
        "(n_x, n_h, n_y) = layer_sizes(X, Y)\n",
        "print(\"The size of the input layer is: n_x = \" + str(n_x))\n",
        "print(\"The size of the hidden layer is: n_h = \" + str(n_h))\n",
        "print(\"The size of the output layer is: n_y = \" + str(n_y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7-t_qVHbj2q"
      },
      "source": [
        "### 3.2 - Initialize the model's parameters ####\n",
        "\n",
        "**Exercise**: Implement the function `initialize_parameters()`.\n",
        "\n",
        "- Make sure your parameters' sizes are right. Refer to the figure above.\n",
        "- Initialize the weights matrices with random values.\n",
        "    - Use: `np.random.randn(a,b) * 0.01` to randomly initialize a matrix of shape (a,b).\n",
        "- You will initialize the bias vectors as zeros.\n",
        "    - Use: `np.zeros((a,b))` to initialize a matrix of shape (a,b) with zeros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "og-6c9ubbj2r"
      },
      "source": [
        "def initialize_parameters(n_x, n_h, n_y):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    n_x -- size of the input layer\n",
        "    n_h -- size of the hidden layer\n",
        "    n_y -- size of the output layer\n",
        "    Returns:\n",
        "    params -- python dictionary containing your parameters:\n",
        "                    W1 -- weight matrix of shape (n_h, n_x)\n",
        "                    b1 -- bias vector of shape (n_h, 1)\n",
        "                    W2 -- weight matrix of shape (n_y, n_h)\n",
        "                    b2 -- bias vector of shape (n_y, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    np.random.seed(2) # we set up a seed so that the output matches the solution although the initialization is random.\n",
        "    print(\"The size of the input layer is: n_x = \" + str(n_x))\n",
        "    print(\"The size of the hidden layer is: n_h = \" + str(n_h))\n",
        "    print(\"The size of the output layer is: n_y = \" + str(n_y))\n",
        "\n",
        "\n",
        "    ### START CODE HERE ### (â‰ˆ 4 lines of code)\n",
        "    W1 = None\n",
        "    b1 = None\n",
        "    W2 = None\n",
        "    b2 = None\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    assert (W1.shape == (n_h, n_x))\n",
        "    assert (b1.shape == (n_h, 1))\n",
        "    assert (W2.shape == (n_y, n_h))\n",
        "    assert (b2.shape == (n_y, 1))\n",
        "\n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "\n",
        "    return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLfs6kORbj2r"
      },
      "source": [
        "n_x, n_h, n_y = 2, 4, 1\n",
        "\n",
        "parameters = initialize_parameters(n_x, n_h, n_y)\n",
        "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
        "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
        "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
        "print(\"b2 = \" + str(parameters[\"b2\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-RL17H5bj2r"
      },
      "source": [
        "### 3.3 - The Loop ####\n",
        "\n",
        "**Exercise**: Implement `forward_propagation()`.\n",
        "\n",
        "- Use the function `sigmoid()`.\n",
        "- You can use the function `np.tanh()`. It is part of the numpy library.\n",
        "- Retrieve each parameter from the dictionary \"parameters\" (which is the output of `initialize_parameters()`) by using `parameters[\"..\"]`.\n",
        "- Implement forward propagation. Compute $Z^{[1]}, A^{[1]}, Z^{[2]}$ and $A^{[2]}$ (the vector of all your predictions on all the examples in the training set).\n",
        "- Values needed in the backpropagation are stored in \"`cache`\". The `cache` will be given as an input to the backpropagation function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kBM-BaCbj2s"
      },
      "source": [
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid of x\n",
        "    Arguments: x -- A scalar or numpy array of any size.\n",
        "    Return: s -- sigmoid(x)\n",
        "    \"\"\"\n",
        "    s = 1/(1+np.exp(-x))\n",
        "    return s\n",
        "\n",
        "def forward_propagation(X, parameters):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    X -- input data of size (n_x, m)\n",
        "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
        "\n",
        "    Returns:\n",
        "    A2 -- The sigmoid output of the second activation\n",
        "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
        "    \"\"\"\n",
        "    # Retrieve each parameter from the dictionary \"parameters\"\n",
        "    ### START CODE HERE ###\n",
        "    W1 = None\n",
        "    b1 = None\n",
        "    W2 = None\n",
        "    b2 = None\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Implement forward propagation to calculate A2 (probabilities)\n",
        "    ### START CODE HERE ###\n",
        "    Z1 = None\n",
        "    A1 = None\n",
        "    Z2 = None\n",
        "    A2 = None\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    assert(A2.shape == (1, X.shape[1]))\n",
        "\n",
        "    cache = {\"Z1\": Z1,\n",
        "             \"A1\": A1,\n",
        "             \"Z2\": Z2,\n",
        "             \"A2\": A2}\n",
        "\n",
        "    return A2, cache"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHFc8OXzNa0T"
      },
      "source": [
        "Test the function on sample data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVhIyxqkbj2s"
      },
      "source": [
        "def forward_propagation_test_case():\n",
        "    np.random.seed(1)\n",
        "    X_assess = np.random.randn(2, 3)\n",
        "    b1 = np.random.randn(4,1)\n",
        "    b2 = np.array([[ -1.3]])\n",
        "\n",
        "    parameters = {'W1': np.array([[-0.00416758, -0.00056267],\n",
        "        [-0.02136196,  0.01640271],\n",
        "        [-0.01793436, -0.00841747],\n",
        "        [ 0.00502881, -0.01245288]]),\n",
        "     'W2': np.array([[-0.01057952, -0.00909008,  0.00551454,  0.02292208]]),\n",
        "     'b1': b1,\n",
        "     'b2': b2}\n",
        "\n",
        "    return X_assess, parameters\n",
        "\n",
        "X_assess, parameters = forward_propagation_test_case()\n",
        "A2, cache = forward_propagation(X_assess, parameters)\n",
        "\n",
        "# Note: we use the mean here just to make sure that your output matches ours.\n",
        "print(np.mean(cache['Z1']) ,np.mean(cache['A1']),np.mean(cache['Z2']),np.mean(cache['A2']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkaOsve2bj2t"
      },
      "source": [
        "Now that you have computed $A^{[2]}$, which contains $a^{[2](i)}$ for every example, you can compute the cost function as follows:\n",
        "\n",
        "$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)} \\small\\tag{13}$$\n",
        "\n",
        "**Exercise**: Implement `compute_cost()` to compute the value of the cost $J$.\n",
        "\n",
        "Use either `np.multiply()` and then `np.sum()` or directly `np.dot()`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyhjED8abj2t"
      },
      "source": [
        "def compute_cost(A2, Y, parameters):\n",
        "    \"\"\"\n",
        "    Computes the cross-entropy cost given in equation (13)\n",
        "    Arguments:\n",
        "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
        "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
        "    parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n",
        "    Returns:\n",
        "    cost -- cross-entropy cost given equation (13)\n",
        "    \"\"\"\n",
        "\n",
        "    m = Y.shape[1] # number of example\n",
        "\n",
        "    # Compute the cross-entropy cost\n",
        "    ### START CODE HERE ###\n",
        "    logprobs = None\n",
        "    cost = None\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    cost = np.squeeze(cost)     # makes sure cost is the dimension we expect.\n",
        "                                # E.g., turns [[17]] into 17\n",
        "    assert(isinstance(cost, float))\n",
        "\n",
        "    return cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdFHjfx_OTIZ"
      },
      "source": [
        "Test the function on sample data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1g5kNDHbj2t"
      },
      "source": [
        "def compute_cost_test_case():\n",
        "    np.random.seed(1)\n",
        "    Y_assess = (np.random.randn(1, 3) > 0)\n",
        "    parameters = {'W1': np.array([[-0.00416758, -0.00056267],\n",
        "        [-0.02136196,  0.01640271],\n",
        "        [-0.01793436, -0.00841747],\n",
        "        [ 0.00502881, -0.01245288]]),\n",
        "     'W2': np.array([[-0.01057952, -0.00909008,  0.00551454,  0.02292208]]),\n",
        "     'b1': np.array([[ 0.],\n",
        "        [ 0.],\n",
        "        [ 0.],\n",
        "        [ 0.]]),\n",
        "     'b2': np.array([[ 0.]])}\n",
        "\n",
        "    a2 = (np.array([[ 0.5002307 ,  0.49985831,  0.50023963]]))\n",
        "\n",
        "    return a2, Y_assess, parameters\n",
        "\n",
        "A2, Y_assess, parameters = compute_cost_test_case()\n",
        "\n",
        "print(\"cost = \" + str(compute_cost(A2, Y_assess, parameters)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ltihqq8Qbj2u"
      },
      "source": [
        "Using the cache computed during forward propagation, you can now implement backward propagation.\n",
        "\n",
        "**Exercise**: Implement the function `backward_propagation()`.\n",
        "\n",
        "Backpropagation is usually the hardest (most mathematical) part in deep learning. Build a vectorized implementation.\n",
        "\n",
        "<img src=\"https://github.com/gemaatienza/Deep-Learning-Coursera/raw/691374a31a373841bd3cad8cfa96fbef0b04a3dc/1.%20Neural%20Networks%20and%20Deep%20Learning/images/grad_summary.png\">\n",
        "\n",
        "To compute $dZ1$ you'll need to compute $g^{[1]'}(Z^{[1]})$. Since $g^{[1]}(.)$ is the tanh activation function, if $a = g^{[1]}(z)$ then $g^{[1]'}(z) = 1-a^2$. So you can compute $g^{[1]'}(Z^{[1]})$ using `(1 - np.power(A1, 2))`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoS6RXFtbj2u"
      },
      "source": [
        "def backward_propagation(parameters, cache, X, Y):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation using the instructions above.\n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing our parameters\n",
        "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
        "    X -- input data of shape (2, number of examples)\n",
        "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
        "    Returns:\n",
        "    grads -- python dictionary containing your gradients with respect to different parameters\n",
        "    \"\"\"\n",
        "    m = X.shape[1]\n",
        "\n",
        "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
        "    ### START CODE HERE ###\n",
        "    W1 = None\n",
        "    W2 = None\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
        "    ### START CODE HERE ###\n",
        "    A1 = None\n",
        "    A2 = None\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Backward propagation: calculate dW1, db1, dW2, db2.\n",
        "    ### START CODE HERE ### (6 equations on slide above)\n",
        "    dZ2 = None\n",
        "    dW2 = None\n",
        "    db2 = None\n",
        "    dZ1 = None\n",
        "    dW1 = None\n",
        "    db1 = None\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    grads = {\"dW1\": dW1,\n",
        "             \"db1\": db1,\n",
        "             \"dW2\": dW2,\n",
        "             \"db2\": db2}\n",
        "\n",
        "    return grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgPGPi6IP2ZI"
      },
      "source": [
        "Test the function on sample data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SK198DATbj2v"
      },
      "source": [
        "def backward_propagation_test_case():\n",
        "    np.random.seed(1)\n",
        "    X_assess = np.random.randn(2, 3)\n",
        "    Y_assess = (np.random.randn(1, 3) > 0)\n",
        "    parameters = {'W1': np.array([[-0.00416758, -0.00056267],\n",
        "        [-0.02136196,  0.01640271],\n",
        "        [-0.01793436, -0.00841747],\n",
        "        [ 0.00502881, -0.01245288]]),\n",
        "     'W2': np.array([[-0.01057952, -0.00909008,  0.00551454,  0.02292208]]),\n",
        "     'b1': np.array([[ 0.],\n",
        "        [ 0.],\n",
        "        [ 0.],\n",
        "        [ 0.]]),\n",
        "     'b2': np.array([[ 0.]])}\n",
        "\n",
        "    cache = {'A1': np.array([[-0.00616578,  0.0020626 ,  0.00349619],\n",
        "         [-0.05225116,  0.02725659, -0.02646251],\n",
        "         [-0.02009721,  0.0036869 ,  0.02883756],\n",
        "         [ 0.02152675, -0.01385234,  0.02599885]]),\n",
        "  'A2': np.array([[ 0.5002307 ,  0.49985831,  0.50023963]]),\n",
        "  'Z1': np.array([[-0.00616586,  0.0020626 ,  0.0034962 ],\n",
        "         [-0.05229879,  0.02726335, -0.02646869],\n",
        "         [-0.02009991,  0.00368692,  0.02884556],\n",
        "         [ 0.02153007, -0.01385322,  0.02600471]]),\n",
        "  'Z2': np.array([[ 0.00092281, -0.00056678,  0.00095853]])}\n",
        "    return parameters, cache, X_assess, Y_assess\n",
        "\n",
        "parameters, cache, X_assess, Y_assess = backward_propagation_test_case()\n",
        "\n",
        "grads = backward_propagation(parameters, cache, X_assess, Y_assess)\n",
        "print (\"dW1 = \"+ str(grads[\"dW1\"]))\n",
        "print (\"db1 = \"+ str(grads[\"db1\"]))\n",
        "print (\"dW2 = \"+ str(grads[\"dW2\"]))\n",
        "print (\"db2 = \"+ str(grads[\"db2\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sc9vRx1nbj2v"
      },
      "source": [
        "**Exercise**: Implement the update rule. Use gradient descent. You have to use (dW1, db1, dW2, db2) in order to update (W1, b1, W2, b2).\n",
        "\n",
        "General gradient descent rule: $ \\theta = \\theta - \\alpha \\frac{\\partial J }{ \\partial \\theta }$ where $\\alpha$ is the learning rate and $\\theta$ represents a parameter.\n",
        "\n",
        "The gradient descent algorithm with a good learning rate (converging) and a bad learning rate (diverging).\n",
        "\n",
        "<img src=\"https://github.com/gemaatienza/Deep-Learning-Coursera/raw/691374a31a373841bd3cad8cfa96fbef0b04a3dc/1.%20Neural%20Networks%20and%20Deep%20Learning/images/sgd.gif\"> <img src=\"https://github.com/gemaatienza/Deep-Learning-Coursera/raw/691374a31a373841bd3cad8cfa96fbef0b04a3dc/1.%20Neural%20Networks%20and%20Deep%20Learning/images/sgd_bad.gif\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGUd5uS7bj2v"
      },
      "source": [
        "def update_parameters(parameters, grads, learning_rate = 1.2):\n",
        "    \"\"\"\n",
        "    Updates parameters using the gradient descent update rule given above\n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters\n",
        "    grads -- python dictionary containing your gradients\n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters\n",
        "    \"\"\"\n",
        "    # Retrieve each parameter from the dictionary \"parameters\"\n",
        "    ### START CODE HERE ###\n",
        "    W1 = None\n",
        "    b1 = None\n",
        "    W2 = None\n",
        "    b2 = None\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Retrieve each gradient from the dictionary \"grads\"\n",
        "    ### START CODE HERE ###\n",
        "    dW1 = None\n",
        "    db1 = None\n",
        "    dW2 = None\n",
        "    db2 = None\n",
        "    ## END CODE HERE ###\n",
        "\n",
        "    # Update rule for each parameter\n",
        "    ### START CODE HERE ###\n",
        "    W1 = None\n",
        "    b1 = None\n",
        "    W2 = None\n",
        "    b2 = None\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "\n",
        "    return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roOp-3soQ1p4"
      },
      "source": [
        "Test the function on sample data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "d3BNZT36bj2w"
      },
      "source": [
        "def update_parameters_test_case():\n",
        "    parameters = {'W1': np.array([[-0.00615039,  0.0169021 ],\n",
        "        [-0.02311792,  0.03137121],\n",
        "        [-0.0169217 , -0.01752545],\n",
        "        [ 0.00935436, -0.05018221]]),\n",
        " 'W2': np.array([[-0.0104319 , -0.04019007,  0.01607211,  0.04440255]]),\n",
        " 'b1': np.array([[ -8.97523455e-07],\n",
        "        [  8.15562092e-06],\n",
        "        [  6.04810633e-07],\n",
        "        [ -2.54560700e-06]]),\n",
        " 'b2': np.array([[  9.14954378e-05]])}\n",
        "\n",
        "    grads = {'dW1': np.array([[ 0.00023322, -0.00205423],\n",
        "        [ 0.00082222, -0.00700776],\n",
        "        [-0.00031831,  0.0028636 ],\n",
        "        [-0.00092857,  0.00809933]]),\n",
        " 'dW2': np.array([[ -1.75740039e-05,   3.70231337e-03,  -1.25683095e-03,\n",
        "          -2.55715317e-03]]),\n",
        " 'db1': np.array([[  1.05570087e-07],\n",
        "        [ -3.81814487e-06],\n",
        "        [ -1.90155145e-07],\n",
        "        [  5.46467802e-07]]),\n",
        " 'db2': np.array([[ -1.08923140e-05]])}\n",
        "    return parameters, grads\n",
        "\n",
        "parameters, grads = update_parameters_test_case()\n",
        "parameters = update_parameters(parameters, grads)\n",
        "\n",
        "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
        "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
        "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
        "print(\"b2 = \" + str(parameters[\"b2\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5g9X4-b4bj2w"
      },
      "source": [
        "### 3.4 - Integrate parts 3.1, 3.2 and 3.3 in nn_model() ####\n",
        "\n",
        "**Exercise**: Build your neural network model in `nn_model()`.\n",
        "\n",
        "The neural network model has to use the previous functions in the right order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZPjiqy1bj2w"
      },
      "source": [
        "def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    X -- dataset of shape (2, number of examples)\n",
        "    Y -- labels of shape (1, number of examples)\n",
        "    n_h -- size of the hidden layer\n",
        "    num_iterations -- Number of iterations in gradient descent loop\n",
        "    print_cost -- if True, print the cost every 1000 iterations\n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
        "    \"\"\"\n",
        "\n",
        "    np.random.seed(3)\n",
        "    n_x = layer_sizes(X, Y)[0]\n",
        "    n_y = layer_sizes(X, Y)[2]\n",
        "\n",
        "    # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: \"n_x, n_h, n_y\". Outputs = \"W1, b1, W2, b2, parameters\".\n",
        "    ### START CODE HERE ###\n",
        "    parameters = None\n",
        "    W1 = None\n",
        "    b1 = None\n",
        "    W2 = None\n",
        "    b2 = None\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Loop (gradient descent)\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        ### START CODE HERE ###\n",
        "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
        "        A2, cache = None\n",
        "\n",
        "        # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n",
        "        cost = None\n",
        "\n",
        "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
        "        grads = None\n",
        "\n",
        "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
        "        parameters = None\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Print the cost every 1000 iterations\n",
        "        if print_cost and i % 1000 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "\n",
        "    return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGzTQOIJRmJ-"
      },
      "source": [
        "Test the function on sample data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT2eYOHwbj2x"
      },
      "source": [
        "def nn_model_test_case():\n",
        "    np.random.seed(1)\n",
        "    X_assess = np.random.randn(2, 3)\n",
        "    Y_assess = (np.random.randn(1, 3) > 0)\n",
        "    return X_assess, Y_assess\n",
        "\n",
        "X_assess, Y_assess = nn_model_test_case()\n",
        "parameters = nn_model(X_assess, Y_assess, 4, num_iterations=10000, print_cost=True)\n",
        "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
        "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
        "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
        "print(\"b2 = \" + str(parameters[\"b2\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6sDOsvMbj2x"
      },
      "source": [
        "### 3.5 Predictions\n",
        "\n",
        "**Exercise**: Use your model to predict by building predict().\n",
        "Use forward propagation to predict results.\n",
        "\n",
        "predictions = $y_{prediction} =$ \\begin{cases}\n",
        "      1 & \\text{if}\\ activation > 0.5 \\\\\n",
        "      0 & \\text{otherwise}\n",
        "    \\end{cases}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyEci_Ldbj2y"
      },
      "source": [
        "def predict(parameters, X):\n",
        "    \"\"\"\n",
        "    Using the learned parameters, predicts a class for each example in X\n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters\n",
        "    X -- input data of size (n_x, m)\n",
        "    Returns\n",
        "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
        "    \"\"\"\n",
        "\n",
        "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
        "    ### START CODE HERE ###\n",
        "    A2, cache = None\n",
        "    threshold=0.5\n",
        "    predictions = None\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7K9ng-bSe4Z"
      },
      "source": [
        "Test the function on sample data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDRM3Hsibj2y"
      },
      "source": [
        "def predict_test_case():\n",
        "    np.random.seed(1)\n",
        "    X_assess = np.random.randn(2, 3)\n",
        "    parameters = {'W1': np.array([[-0.00615039,  0.0169021 ],\n",
        "        [-0.02311792,  0.03137121],\n",
        "        [-0.0169217 , -0.01752545],\n",
        "        [ 0.00935436, -0.05018221]]),\n",
        "     'W2': np.array([[-0.0104319 , -0.04019007,  0.01607211,  0.04440255]]),\n",
        "     'b1': np.array([[ -8.97523455e-07],\n",
        "        [  8.15562092e-06],\n",
        "        [  6.04810633e-07],\n",
        "        [ -2.54560700e-06]]),\n",
        "     'b2': np.array([[  9.14954378e-05]])}\n",
        "    return parameters, X_assess\n",
        "\n",
        "parameters, X_assess = predict_test_case()\n",
        "\n",
        "predictions = predict(parameters, X_assess)\n",
        "print(\"predictions mean = \" + str(np.mean(predictions)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbRVMK7Mbj2y"
      },
      "source": [
        "Let's see how the model performs on a planar dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "gLbHwUXtbj2y"
      },
      "source": [
        "# Build a model with a n_h-dimensional hidden layer\n",
        "parameters = nn_model(X, Y, n_h = 4, num_iterations = 10000, print_cost=True)\n",
        "\n",
        "# Plot the decision boundary\n",
        "plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\n",
        "plt.title(\"Decision Boundary for hidden layer size \" + str(4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyrKdoCCbj2z"
      },
      "source": [
        "# Print accuracy\n",
        "predictions = predict(parameters, X)\n",
        "print ('Accuracy: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-34aPS3jbj2z"
      },
      "source": [
        "Accuracy is really high compared to Logistic Regression. The model has learnt the leaf patterns of the flower! Neural networks are able to learn even highly non-linear decision boundaries, unlike logistic regression.\n",
        "\n",
        "Let's try out several hidden layer sizes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhdkncXLbj2z"
      },
      "source": [
        "### 3.6 - Tuning hidden layer size ###\n",
        "\n",
        "Run the following code. It may take 1-2 minutes. You will observe different behaviors of the model for various hidden layer sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "kiUNHY-Ebj20"
      },
      "source": [
        "# This may take about 2 minutes to run\n",
        "\n",
        "plt.figure(figsize=(16, 32))\n",
        "hidden_layer_sizes = [1, 2, 3, 4, 5, 20, 50]\n",
        "for i, n_h in enumerate(hidden_layer_sizes):\n",
        "    plt.subplot(5, 2, i+1)\n",
        "    plt.title('Hidden Layer of size %d' % n_h)\n",
        "    parameters = nn_model(X, Y, n_h, num_iterations = 10000)\n",
        "    plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\n",
        "    predictions = predict(parameters, X)\n",
        "    accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100)\n",
        "    print (\"Accuracy for {} hidden units: {} %\".format(n_h, accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8wBVjvibj20"
      },
      "source": [
        "The larger models (with more hidden units) are able to fit the training set better, until eventually the largest models overfit the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3itAjUpbj21"
      },
      "source": [
        "References:\n",
        "- http://scs.ryerson.ca/~aharley/neural-networks/\n",
        "- http://cs231n.github.io/neural-networks-case-study/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ani6npUjs5Ty"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}