{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Beyond Black Boxes Tutorial"
      ],
      "metadata": {
        "id": "4-sKmVlkbw8U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Manual Understanding\n"
      ],
      "metadata": {
        "id": "Gzu78I9JcGqf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xw_Pzwsabpp0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import googlenet\n",
        "\n",
        "model = googlenet(pretrained=True).to(device)"
      ],
      "metadata": {
        "id": "lI_ccXX7qUq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GoogleNet architecture: https://github.com/pytorch/vision/blob/main/torchvision/models/googlenet.py\n",
        "\n",
        "class NewModel(nn.Module):\n",
        "\n",
        "\tdef __init__(self, model):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.model = model\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\t\t# N x 3 x 224 x 224\n",
        "\t\t\tx = self.model.conv1(x)\n",
        "\t\t\t# N x 64 x 112 x 112\n",
        "\t\t\tx = self.model.maxpool1(x)\n",
        "\t\t\t# N x 64 x 56 x 56\n",
        "\t\t\tx = self.model.conv2(x)\n",
        "\t\t\t# N x 64 x 56 x 56\n",
        "\t\t\tx = self.model.conv3(x)\n",
        "\t\t\t# N x 192 x 56 x 56\n",
        "\t\t\tx = self.model.maxpool2(x)\n",
        "\t\t\t# N x 192 x 28 x 28\n",
        "\t\t\tx = self.model.inception3a(x)\n",
        "\t\t\t# N x 256 x 28 x 28\n",
        "\t\t\tx = self.model.inception3b(x)\n",
        "\t\t\t# N x 480 x 28 x 28\n",
        "\t\t\tx = self.model.maxpool3(x)\n",
        "\t\t\t# N x 480 x 14 x 14\n",
        "\t\t\tx = self.model.inception4a(x)\n",
        "\t\t\t# N x 512 x 14 x 14\n",
        "\t\t\tx = self.model.inception4b(x)\n",
        "\t\t\t# N x 512 x 14 x 14\n",
        "\t\t\tx = self.model.inception4c(x)\n",
        "\t\t\t# N x 512 x 14 x 14\n",
        "\t\t\tx = self.model.inception4d(x)\n",
        "\t\t\t# N x 528 x 14 x 14\n",
        "\t\t\tx = self.model.inception4e(x)\n",
        "\t\t\t# N x 832 x 14 x 14\n",
        "\t\t\tx = self.model.maxpool4(x)\n",
        "\t\t\t# N x 832 x 7 x 7\n",
        "\t\t\tx = self.model.inception5a(x)\n",
        "\t\t\t# N x 832 x 7 x 7\n",
        "\t\t\t# x = self.model.inception5b(x)\n",
        "\t\t\t# # N x 1024 x 7 x 7\n",
        "\t\t\t# x = self.model.avgpool(x)\n",
        "\t\t\t# # N x 1024 x 1 x 1\n",
        "\t\t\t# x = torch.flatten(x, 1)\n",
        "\t\t\t# # N x 1024\n",
        "\t\t\t# x = self.model.dropout(x)\n",
        "\t\t\t# x = self.model.fc(x)\n",
        "\t\t\t# N x 1000 (num_classes)\n",
        "\t\t\treturn x"
      ],
      "metadata": {
        "id": "1pmNSi_8qmwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newmodel = NewModel(model)\n",
        "newmodel.eval()\n",
        "\n",
        "seed = 999\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)"
      ],
      "metadata": {
        "id": "EYbCLfbaq1Mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def layer_gradient(model, input_tensor, lrs, sigmas, index=0):\n",
        "\t\tinput_tensor = input_tensor.to(device)\n",
        "\t\titerations = 100\n",
        "\n",
        "\t\tfor i in range(iterations):\n",
        "\t\t\t\tinput_tensor.requires_grad = True\n",
        "\t\t\t\toutput = model(input_tensor).to(device)\n",
        "\n",
        "\t\t\t\tfocus = output[0, index, :, :] # the target to maximize the output\n",
        "\t\t\t\ttarget = torch.ones(focus.shape).to(device) * 200 # make a large target of the correct dims\n",
        "\t\t\t\tloss = torch.sum(target - focus)\n",
        "\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\tinput_tensor = input_tensor - (lrs[0]*(iterations-i)/iterations + lrs[1]*i/iterations) * input_tensor.grad\n",
        "\t\t\t\tinput_tensor = torchvision.transforms.functional.gaussian_blur(\n",
        "\t\t\t\t\t\t input_tensor, 3, sigma=(sigmas[0]*(iterations-i)/iterations + sigmas[1]*i/iterations))\n",
        "\n",
        "\t\t\t\tinput_tensor = input_tensor.detach()\n",
        "\n",
        "\t\treturn input_tensor"
      ],
      "metadata": {
        "id": "9xmPIrp3q9lJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = 5\n",
        "\n",
        "input_tensor = (torch.rand(1, 3, 299, 299))\n",
        "ouput_tensor = layer_gradient(newmodel, input_tensor, [0.5, 0.4], [2.4, 0.8], index)\n",
        "\n",
        "# input_tensor = torchvision.transforms.Resize([380, 380])(input_tensor)\n",
        "# input_tensor = layer_gradient(newmodel, input_tensor, [0.4, 0.3], [1.5, 0.4], index)\n",
        "\n",
        "# input_tensor = torchvision.transforms.Resize([460, 460])(input_tensor)\n",
        "# ouput_tensor = layer_gradient(newmodel, input_tensor, [0.3, 0.2], [1.1, 0.3], index)\n",
        "\n",
        "ouput_tensor = torch.clamp(ouput_tensor, 0, 1)\n",
        "\n",
        "# visualize\n",
        "plt.imshow(ouput_tensor.data[0].cpu().detach().numpy().transpose(1, 2, 0))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W-H4vBlrrG7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Lucent library\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/greentfrapp/lucent/master/images/lucent_header.jpg\" width=\"600\"></img>"
      ],
      "metadata": {
        "id": "hcUlt1Hwb_gl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basics"
      ],
      "metadata": {
        "id": "X8QeXQ64l-R1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet git+https://github.com/greentfrapp/lucent.git"
      ],
      "metadata": {
        "id": "KYvstX1ucOwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from lucent.optvis import render, param, transform, objectives\n",
        "from lucent.modelzoo.util import get_model_layers\n",
        "\n",
        "from torchvision.models import googlenet, resnet50"
      ],
      "metadata": {
        "id": "qmYmhER5c_JC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = resnet50(pretrained=True).to(device).eval()\n",
        "model = googlenet(pretrained=True).to(device).eval()"
      ],
      "metadata": {
        "id": "c2_yOW8HdjPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use `get_model_layers` to retrieve all the model layers."
      ],
      "metadata": {
        "id": "kx1mVyYleXxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first 10 layer names\n",
        "get_model_layers(model)[-10:]"
      ],
      "metadata": {
        "id": "6HfsozHgd8bB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def show_result(result):\n",
        "  result = result[0].squeeze()\n",
        "  print(result.shape)\n",
        "\n",
        "  plt.figure(figsize=(5, 5))\n",
        "  plt.imshow(result)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "GWwlFrQHek_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we can just use `render.render_vis` to generate a visualization of a channel at a particular layer."
      ],
      "metadata": {
        "id": "QMBXxiyqejPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_result(render.render_vis(model, \"inception5a:5\", show_inline=False))"
      ],
      "metadata": {
        "id": "0r5m2KdrfFBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also try to optimize for a particular label by just passing \"labels\" as the layer name, which will use the last layer in the model.\n",
        "\n",
        "ImageNet label IDs are [here](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a)."
      ],
      "metadata": {
        "id": "kL8aagVvgYl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We can try to optimize for \"tiger cat\" which has label ID 282 for this model\n",
        "show_result(render.render_vis(model, \"labels:282\", show_inline=False))"
      ],
      "metadata": {
        "id": "1w8MMVj5gHgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unfortunately we don't get very good results, because we are optimizing the output of a softmax layer.\n",
        "\n",
        "To maximize a particular softmax output, the model can choose to maximize the corresponding input, minimize all other inputs or some combination in between.  We would get a much better visualization if we can directly maximize the logits instead, which we can for the `torchvision` models!"
      ],
      "metadata": {
        "id": "6IIzEbbLhV9_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fiddling with the Knobs\n",
        "\n",
        "Lucent splits visualizations into *objectives*, *parameterizations* and *transforms*:\n",
        "\n",
        "* **Objectives** -- What do you want the model to visualize?\n",
        "* **Parameterizations** -- How do you describe the image?\n",
        "* **Transforms** -- What transformations do you want your visualization to be robust to?"
      ],
      "metadata": {
        "id": "Wnt_KMNel02M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Objectives\n",
        "\n",
        "What loss function do we want to minimize? What part of the model do we want to understand? In essence, we are trying to generate an image that causes a particular neuron or filter to activate strongly. The objective allows us to select a specific neuron, channel or a mix!"
      ],
      "metadata": {
        "id": "_H_Z_diwmFGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The default is optimizing for a channel/filter\n",
        "obj = objectives.channel(\"inception4a\", 248)\n",
        "show_result(render.render_vis(model, obj, show_inline=False))"
      ],
      "metadata": {
        "id": "by7gceI7DRji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The default is optimizing for a channel/filter\n",
        "obj = objectives.channel(\"inception4a\", 261)\n",
        "show_result(render.render_vis(model, obj, show_inline=False))"
      ],
      "metadata": {
        "id": "4e52UzIImL4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The default is optimizing for a channel/filter\n",
        "obj = objectives.neuron(\"inception4a\", 261, 5)\n",
        "show_result(render.render_vis(model, obj, show_inline=False))"
      ],
      "metadata": {
        "id": "p0P_erEImTv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can sum objectives: we sum the previous two objectives and get a mix of the two images\n",
        "channel = lambda n: objectives.channel(\"inception4a\", n)\n",
        "obj = channel(261) + channel(248)\n",
        "show_result(render.render_vis(model, obj, show_inline=False))"
      ],
      "metadata": {
        "id": "J-Kek8QzmffP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Transformations\n",
        "\n",
        "Another way to reduce high-frequency components in the visualization is by imposing constraints in the form of transformation robustness.\n",
        "\n",
        "Read more about this in [The Enemy of Feature Visualization](https://distill.pub/2017/feature-visualization/#enemy-of-feature-vis) from Distill's Feature Visualization article."
      ],
      "metadata": {
        "id": "EXISp8zbnNLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# No transformations, similar to our example earlier\n",
        "obj = objectives.channel(\"inception4a\", 261)\n",
        "show_result(render.render_vis(model, obj, transforms=[], show_inline=False))"
      ],
      "metadata": {
        "id": "pVkNIL44nQgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding jitter, notice that the visualization is much less noisy!\n",
        "jitter_only = [transform.jitter(8)]\n",
        "show_result(render.render_vis(model, obj, transforms=jitter_only, show_inline=False))"
      ],
      "metadata": {
        "id": "Bsh0WCFvnUKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding a whole suite of transforms!\n",
        "all_transforms = [\n",
        "    transform.pad(16),\n",
        "    transform.jitter(8),\n",
        "    transform.random_scale([n/100. for n in range(80, 120)]),\n",
        "    transform.random_rotate(list(range(-10,10)) + list(range(-5,5)) + 10*list(range(-2,2))),\n",
        "    transform.jitter(2),\n",
        "]\n",
        "\n",
        "show_result(render.render_vis(model, obj, transforms=all_transforms, show_inline=False))"
      ],
      "metadata": {
        "id": "Q4V19AQRnXZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And that's it for now!\n",
        "\n",
        "Credit and reference: https://github.com/greentfrapp/lucent"
      ],
      "metadata": {
        "id": "mL02zGSqlgd5"
      }
    }
  ]
}