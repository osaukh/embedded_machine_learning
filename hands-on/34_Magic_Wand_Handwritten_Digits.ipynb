{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BtkMGSYQOTQ"
      },
      "source": [
        "# Magic Wand - Handwritten Digits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaFfr7DHRmGF"
      },
      "source": [
        "This notebook demonstrates how to train a 30kb gesture recognition model for [TensorFlow Lite for Microcontrollers](https://tensorflow.org/lite/microcontrollers/overview). It will produce a model used in the [magic_wand](https://github.com/tensorflow/tflite-micro-arduino-examples/tree/main/examples/magic_wand) example application.\n",
        "\n",
        "Training is much faster using GPU acceleration. Before you proceed, ensure you are using a GPU runtime by going to **Runtime -> Change runtime type** and selecting **GPU**. Training will take around 5 minutes on a GPU runtime.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LG6ErX5FRIaV"
      },
      "source": [
        "## Load imports and configure defaults\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoG0fAqSbqHM"
      },
      "source": [
        "SAVED_MODEL_FILENAME = \"saved_model\"\n",
        "FLOAT_TFL_MODEL_FILENAME = \"float_model.tfl\"\n",
        "QUANTIZED_TFL_MODEL_FILENAME = \"quantized_model.tfl\"\n",
        "TFL_CC_MODEL_FILENAME = \"magic_wand_model_data.cc\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import PIL\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "\n",
        "from IPython.display import Image, display"
      ],
      "metadata": {
        "id": "voGQ9owqGvnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXI7R4RehFdU"
      },
      "source": [
        "## Download and normalize the data\n",
        "\n",
        "Next, we'll download the data and extract it into the expected location within the training scripts' directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2Sg2AKzVr2L"
      },
      "source": [
        "!curl -L https://github.com/petewarden/magic_wand_digit_data/archive/8170591863f9addca27b1a963263f7c7bed33f41.zip -o magic_wand_digit_data.zip\n",
        "!unzip magic_wand_digit_data.zip\n",
        "!rm -rf magic_wand_digit_data\n",
        "!mv magic_wand_digit_data-* magic_wand_digit_data\n",
        "!rm -rf magic_wand_digit_data.zip\n",
        "!rm -rf sample_data\n",
        "!mkdir -p checkpoints"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEIFiaA0aAAj"
      },
      "source": [
        "Read data in the JSON format.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWc-4PNRcNNH"
      },
      "source": [
        "import glob\n",
        "import json\n",
        "\n",
        "strokes = []\n",
        "for filename in glob.glob(\"magic_wand_digit_data/*.json\"):\n",
        "  with open(filename, \"r\") as file:\n",
        "    file_contents = file.read()\n",
        "  file_data = json.loads(file_contents)\n",
        "  for stroke in file_data[\"strokes\"]:\n",
        "    stroke[\"filename\"] = filename\n",
        "    strokes.append(stroke)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYtH6SD3fs6y"
      },
      "source": [
        "The input data we previously downloaded contains accelerometer data from a person performing gestures corresponding to digits 0--9. The code below visualizes the random gesture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4EC7ajLY09-"
      },
      "source": [
        "def plot_stroke(stroke):\n",
        "  x_array = []\n",
        "  y_array = []\n",
        "  for coords in stroke[\"strokePoints\"]:\n",
        "    x_array.append(coords[\"x\"])\n",
        "    y_array.append(coords[\"y\"])\n",
        "\n",
        "  fig = plt.figure(figsize=(12.8, 4.8))\n",
        "  fig.suptitle(stroke[\"label\"])\n",
        "\n",
        "  ax = fig.add_subplot(131)\n",
        "  ax.set_xlabel('x')\n",
        "  ax.set_ylabel('y')\n",
        "  ax.set_xlim(-0.4, 0.4)\n",
        "  ax.set_ylim(-0.4, 0.4)\n",
        "  ax.plot(x_array, y_array)\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "shuffled_strokes = list(strokes)\n",
        "np.random.shuffle(shuffled_strokes)\n",
        "plot_stroke(shuffled_strokes[0])\n",
        "print(f\"This stroke: {len(shuffled_strokes[0]['strokePoints'])}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below contains functions used to pre-process the input data."
      ],
      "metadata": {
        "id": "G6jJ-fXLHwvq"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBqSVpi6Vxss"
      },
      "source": [
        "FIXED_POINT = 4096\n",
        "\n",
        "def mul_fp(a, b):\n",
        "  return (a * b) // FIXED_POINT\n",
        "\n",
        "def div_fp(a, b):\n",
        "  if b == 0:\n",
        "    b = 1\n",
        "  return (a * FIXED_POINT) // b\n",
        "\n",
        "def float_to_fp(a):\n",
        "  return math.floor(a * FIXED_POINT)\n",
        "\n",
        "def norm_to_coord_fp(a, range_fp, half_size_fp):\n",
        "  a_fp = float_to_fp(a)\n",
        "  norm_fp = div_fp(a_fp, range_fp)\n",
        "  return mul_fp(norm_fp, half_size_fp) + half_size_fp\n",
        "\n",
        "def round_fp_to_int(a):\n",
        "  return math.floor((a + (FIXED_POINT / 2)) / FIXED_POINT)\n",
        "\n",
        "def gate(a, min, max):\n",
        "  if a < min:\n",
        "    return min\n",
        "  elif a > max:\n",
        "    return max\n",
        "  else:\n",
        "    return a\n",
        "\n",
        "def rasterize_stroke(stroke_points, x_range, y_range, width, height):\n",
        "  num_channels = 3\n",
        "  buffer_byte_count = height * width * num_channels\n",
        "  buffer = bytearray(buffer_byte_count)\n",
        "\n",
        "  width_fp = width * FIXED_POINT\n",
        "  height_fp = height * FIXED_POINT\n",
        "  half_width_fp = width_fp / 2\n",
        "  half_height_fp = height_fp / 2\n",
        "  x_range_fp = float_to_fp(x_range)\n",
        "  y_range_fp = float_to_fp(y_range)\n",
        "\n",
        "  t_inc_fp = FIXED_POINT // len(stroke_points)\n",
        "\n",
        "  one_half_fp = (FIXED_POINT / 2)\n",
        "\n",
        "  for point_index in range(len(stroke_points) - 1):\n",
        "    start_point = stroke_points[point_index]\n",
        "    end_point = stroke_points[point_index + 1]\n",
        "    start_x_fp = norm_to_coord_fp(start_point[\"x\"], x_range_fp, half_width_fp)\n",
        "    start_y_fp = norm_to_coord_fp(-start_point[\"y\"], y_range_fp, half_height_fp)\n",
        "    end_x_fp = norm_to_coord_fp(end_point[\"x\"], x_range_fp, half_width_fp)\n",
        "    end_y_fp = norm_to_coord_fp(-end_point[\"y\"], y_range_fp, half_height_fp)\n",
        "    delta_x_fp = end_x_fp - start_x_fp\n",
        "    delta_y_fp = end_y_fp - start_y_fp\n",
        "\n",
        "    t_fp = point_index * t_inc_fp\n",
        "    if t_fp < one_half_fp:\n",
        "      local_t_fp = div_fp(t_fp, one_half_fp)\n",
        "      one_minus_t_fp = FIXED_POINT - local_t_fp\n",
        "      red = round_fp_to_int(one_minus_t_fp * 255)\n",
        "      green = round_fp_to_int(local_t_fp * 255)\n",
        "      blue = 0\n",
        "    else:\n",
        "      local_t_fp = div_fp(t_fp - one_half_fp, one_half_fp)\n",
        "      one_minus_t_fp = FIXED_POINT - local_t_fp\n",
        "      red = 0\n",
        "      green = round_fp_to_int(one_minus_t_fp * 255)\n",
        "      blue = round_fp_to_int(local_t_fp * 255)\n",
        "    red = gate(red, 0, 255)\n",
        "    green = gate(green, 0, 255)\n",
        "    blue = gate(blue, 0, 255)\n",
        "\n",
        "    if abs(delta_x_fp) > abs(delta_y_fp):\n",
        "      line_length = abs(round_fp_to_int(delta_x_fp))\n",
        "      if delta_x_fp > 0:\n",
        "        x_inc_fp = 1 * FIXED_POINT\n",
        "        y_inc_fp = div_fp(delta_y_fp, delta_x_fp)\n",
        "      else:\n",
        "        x_inc_fp = -1 * FIXED_POINT\n",
        "        y_inc_fp = -div_fp(delta_y_fp, delta_x_fp)\n",
        "    else:\n",
        "      line_length = abs(round_fp_to_int(delta_y_fp))\n",
        "      if delta_y_fp > 0:\n",
        "        y_inc_fp = 1 * FIXED_POINT\n",
        "        x_inc_fp = div_fp(delta_x_fp, delta_y_fp)\n",
        "      else:\n",
        "        y_inc_fp = -1 * FIXED_POINT\n",
        "        x_inc_fp = -div_fp(delta_x_fp, delta_y_fp)\n",
        "    for i in range(line_length + 1):\n",
        "      x_fp = start_x_fp + (i * x_inc_fp)\n",
        "      y_fp = start_y_fp + (i * y_inc_fp)\n",
        "      x = round_fp_to_int(x_fp)\n",
        "      y = round_fp_to_int(y_fp)\n",
        "      if (x < 0) or (x >= width) or (y < 0) or (y >= height):\n",
        "        continue\n",
        "      buffer_index = (y * width * num_channels) + (x * num_channels)\n",
        "      buffer[buffer_index + 0] = red\n",
        "      buffer[buffer_index + 1] = green\n",
        "      buffer[buffer_index + 2] = blue\n",
        "  \n",
        "  np_buffer = np.frombuffer(buffer, dtype=np.uint8).reshape(height, width, num_channels)\n",
        "\n",
        "  return np_buffer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xh6wbn3S6o1v"
      },
      "source": [
        "raster = rasterize_stroke(shuffled_strokes[0][\"strokePoints\"], 0.5, 0.5, 32, 32)\n",
        "img = PIL.Image.fromarray(raster).resize((256, 256), PIL.Image.NEAREST)\n",
        "display(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data augmentation"
      ],
      "metadata": {
        "id": "HVR4kxuSxKB3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kU-z47VHdl-u"
      },
      "source": [
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "X_RANGE = 0.6\n",
        "Y_RANGE = 0.6\n",
        "\n",
        "def ensure_empty_dir(dirname):\n",
        "  dirpath = Path(dirname)\n",
        "  if dirpath.exists() and dirpath.is_dir():\n",
        "    shutil.rmtree(dirpath)\n",
        "  dirpath.mkdir()\n",
        "\n",
        "def augment_points(points, move_range, scale_range, rotate_range):\n",
        "  move_x = np.random.uniform(low=-move_range, high=move_range)\n",
        "  move_y = np.random.uniform(low=-move_range, high=move_range)\n",
        "  scale = np.random.uniform(low=1.0-scale_range, high=1.0+scale_range)\n",
        "  rotate = np.random.uniform(low=-rotate_range, high=rotate_range)\n",
        "\n",
        "  x_axis_x = math.cos(rotate) * scale\n",
        "  x_axis_y = math.sin(rotate) * scale\n",
        "\n",
        "  y_axis_x = -math.sin(rotate) * scale\n",
        "  y_axis_y = math.cos(rotate) * scale\n",
        "\n",
        "  new_points = []\n",
        "  for point in points:\n",
        "    old_x = point[\"x\"]\n",
        "    old_y = point[\"y\"]\n",
        "    new_x = (x_axis_x * old_x) + (x_axis_y * old_y) + move_x\n",
        "    new_y = (y_axis_x * old_x) + (y_axis_y * old_y) + move_y\n",
        "    new_points.append({\"x\": new_x, \"y\": new_y})\n",
        "\n",
        "  return new_points\n",
        "\n",
        "def save_strokes_as_images(strokes, root_folder, width, height, augment_count):\n",
        "  ensure_empty_dir(root_folder)\n",
        "  labels = set()\n",
        "  for stroke in strokes:\n",
        "    labels.add(stroke[\"label\"].lower())\n",
        "  for label in labels:\n",
        "    label_path = Path(root_folder, label)\n",
        "    ensure_empty_dir(label_path)\n",
        "\n",
        "  label_counts = {}\n",
        "  for stroke in strokes:\n",
        "    points = stroke[\"strokePoints\"]\n",
        "    label = stroke[\"label\"].lower()\n",
        "    if label == \"\":\n",
        "      raise Exception(\"Missing label for %s:%d\" % (stroke[\"filename\"], stroke[\"index\"]))\n",
        "    if label not in label_counts:\n",
        "      label_counts[label] = 0\n",
        "    label_count = label_counts[label]\n",
        "    label_counts[label] += 1\n",
        "    raster = rasterize_stroke(points, X_RANGE, Y_RANGE, width, height)\n",
        "    image = PIL.Image.fromarray(raster)\n",
        "    image.save(Path(root_folder, label, str(label_count) + \".png\"))\n",
        "    for i in range(augment_count):\n",
        "      # Note: no move augmentation as the stroke should be more or less\n",
        "      # centered within the raster\n",
        "      augmented_points = augment_points(points, 0.0, 0.1, 0.3)\n",
        "      raster = rasterize_stroke(augmented_points, X_RANGE, Y_RANGE, width, height)\n",
        "      image = PIL.Image.fromarray(raster)\n",
        "      image.save(Path(root_folder, label, str(label_count) + \"_a\" + str(i) + \".png\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZDL4IFwChpG"
      },
      "source": [
        "## Data split: 80-10-10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLOwvhuTdcGu"
      },
      "source": [
        "IMAGE_WIDTH = 32\n",
        "IMAGE_HEIGHT = 32\n",
        "\n",
        "shuffled_strokes = list(strokes)\n",
        "np.random.shuffle(shuffled_strokes)\n",
        "\n",
        "test_percentage = 10\n",
        "validation_percentage = 10\n",
        "train_percentage = 100 - (test_percentage + validation_percentage)\n",
        "\n",
        "test_count = math.floor((len(shuffled_strokes) * test_percentage) / 100)\n",
        "validation_count = math.floor((len(shuffled_strokes) * validation_percentage) / 100)\n",
        "test_strokes = shuffled_strokes[0:test_count]\n",
        "validation_strokes = shuffled_strokes[test_count:(test_count + validation_count)]\n",
        "train_strokes = shuffled_strokes[(test_count + validation_count):]\n",
        "\n",
        "save_strokes_as_images(test_strokes, \"test\", IMAGE_WIDTH, IMAGE_HEIGHT, 10)\n",
        "save_strokes_as_images(validation_strokes, \"validation\", IMAGE_WIDTH, IMAGE_HEIGHT, 0)\n",
        "save_strokes_as_images(train_strokes, \"train\", IMAGE_WIDTH, IMAGE_HEIGHT, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCx6SN9NWRPw"
      },
      "source": [
        "validation_ds = image_dataset_from_directory(\n",
        "    directory='validation',\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    batch_size=32,\n",
        "    image_size=(IMAGE_WIDTH, IMAGE_HEIGHT)).prefetch(buffer_size=32)\n",
        "\n",
        "train_ds = image_dataset_from_directory(\n",
        "    directory='train',\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    batch_size=32,\n",
        "    image_size=(IMAGE_WIDTH, IMAGE_HEIGHT)).prefetch(buffer_size=32)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_ds.take(1):\n",
        "  for i in range(9):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    ax.set_title(f\"{np.argmax(labels[i])}\")\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "    plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "l3Mqaiffq8fX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3nvTVDLekmi"
      },
      "source": [
        "## Build and train a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXmQZgbuWQFO"
      },
      "source": [
        "from keras import layers\n",
        "\n",
        "def make_model(input_shape, num_classes):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "\n",
        "    # Entry block\n",
        "    x = layers.Rescaling(1.0 / 255)(inputs)\n",
        "    x = layers.Conv2D(16, 3, strides=2, padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "\n",
        "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "\n",
        "    x = layers.Conv2D(64, 3, strides=2, padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    activation = \"softmax\"\n",
        "    units = num_classes\n",
        "\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(units, activation=activation)(x)\n",
        "    return keras.Model(inputs, outputs)\n",
        "\n",
        "model = make_model(input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 3), num_classes=10)\n",
        "keras.utils.plot_model(model, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 30\n",
        "\n",
        "callbacks = [keras.callbacks.ModelCheckpoint(\"checkpoints/save_at_{epoch}.h5\"),]\n",
        "model.compile(optimizer=keras.optimizers.Adam(1e-3), loss=\"binary_crossentropy\", metrics=[\"accuracy\"],)\n",
        "model.fit(train_ds, epochs=epochs, callbacks=callbacks, validation_data=validation_ds,)"
      ],
      "metadata": {
        "id": "9B5XD6yIrNQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test prediction quality"
      ],
      "metadata": {
        "id": "3WHMHhaUM4mb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_image(model, filename):\n",
        "  img = keras.preprocessing.image.load_img(filename, target_size=(IMAGE_WIDTH, IMAGE_HEIGHT))\n",
        "  img_array = keras.preprocessing.image.img_to_array(img)\n",
        "  img_array = tf.expand_dims(img_array, 0)  # Create batch axis\n",
        "  predictions = model.predict(img_array).flatten()\n",
        "  predicted_label_index = np.argmax(predictions)\n",
        "  predicted_score = predictions[predicted_label_index]\n",
        "  return (predicted_label_index, predicted_score)\n",
        "  \n",
        "index, score = predict_image(model, \"test/7/2.png\")\n",
        "\n",
        "print(index, score)"
      ],
      "metadata": {
        "id": "ZQkjUqQArRVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SCORE_THRESHOLD = 0.75\n",
        "\n",
        "correct_count = 0\n",
        "wrong_count = 0\n",
        "discarded_count = 0\n",
        "for label_dir in glob.glob(\"test/*\"):\n",
        "  label = int(label_dir.replace(\"test/\", \"\"))\n",
        "  for filename in glob.glob(label_dir + \"/*.png\"):\n",
        "    index, score = predict_image(model, filename)\n",
        "    if score < SCORE_THRESHOLD:\n",
        "      discarded_count += 1\n",
        "      continue\n",
        "    if index == label:\n",
        "      correct_count += 1\n",
        "    else:\n",
        "      wrong_count += 1\n",
        "      print(\"%d expected, %d found with score %f\" % (label, index, score))\n",
        "      display(Image(filename=filename))\n",
        "\n",
        "correct_percentage = (correct_count / (correct_count + wrong_count)) * 100\n",
        "print(\"%.1f%% correct (N=%d, %d unknown)\" % (correct_percentage, (correct_count + wrong_count), discarded_count))"
      ],
      "metadata": {
        "id": "cCuKFkfkrYKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(SAVED_MODEL_FILENAME)"
      ],
      "metadata": {
        "id": "e4Qx0Lp8rbgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab40Khc2exks"
      },
      "source": [
        "## Convert TFLite Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kS8yvUUvOwGY"
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_FILENAME)\n",
        "model_no_quant_tflite = converter.convert()\n",
        "\n",
        "# Save the model to disk\n",
        "open(FLOAT_TFL_MODEL_FILENAME, \"wb\").write(model_no_quant_tflite)\n",
        "\n",
        "def representative_dataset():\n",
        "  for filename in glob.glob(\"test/*/*.png\"):\n",
        "    img = keras.preprocessing.image.load_img(filename, target_size=(IMAGE_WIDTH, IMAGE_HEIGHT))\n",
        "    img_array = keras.preprocessing.image.img_to_array(img)\n",
        "    img_array = tf.expand_dims(img_array, 0)  # Create batch axis for images, labels in train_ds.take(1):\n",
        "    yield([img_array])\n",
        "# Set the optimization flag.\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# Enforce integer only quantization\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8\n",
        "# Provide a representative dataset to ensure we quantize correctly.\n",
        "converter.representative_dataset = representative_dataset\n",
        "model_tflite = converter.convert()\n",
        "\n",
        "# Save the model to disk\n",
        "open(QUANTIZED_TFL_MODEL_FILENAME, \"wb\").write(model_tflite)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_tflite(tflite_model, filename):\n",
        "  img = keras.preprocessing.image.load_img(filename, target_size=(IMAGE_WIDTH, IMAGE_HEIGHT))\n",
        "  img_array = keras.preprocessing.image.img_to_array(img)\n",
        "  img_array = tf.expand_dims(img_array, 0)\n",
        "\n",
        "  # Initialize the TFLite interpreter\n",
        "  interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "  interpreter.allocate_tensors()\n",
        "\n",
        "  input_details = interpreter.get_input_details()[0]\n",
        "  output_details = interpreter.get_output_details()[0]\n",
        "\n",
        "  # If required, quantize the input layer (from float to integer)\n",
        "  input_scale, input_zero_point = input_details[\"quantization\"]\n",
        "  if (input_scale, input_zero_point) != (0.0, 0):\n",
        "    img_array = np.multiply(img_array, 1.0 / input_scale) + input_zero_point\n",
        "    img_array = img_array.astype(input_details[\"dtype\"])\n",
        "  \n",
        "  # Invoke the interpreter\n",
        "  interpreter.set_tensor(input_details[\"index\"], img_array)\n",
        "  interpreter.invoke()\n",
        "  pred = interpreter.get_tensor(output_details[\"index\"])[0]\n",
        "  \n",
        "  # If required, dequantized the output layer (from integer to float)\n",
        "  output_scale, output_zero_point = output_details[\"quantization\"]\n",
        "  if (output_scale, output_zero_point) != (0.0, 0):\n",
        "    pred = pred.astype(np.float32)\n",
        "    pred = np.multiply((pred - output_zero_point), output_scale)\n",
        "  \n",
        "  predicted_label_index = np.argmax(pred)\n",
        "  predicted_score = pred[predicted_label_index]\n",
        "  return (predicted_label_index, predicted_score)"
      ],
      "metadata": {
        "id": "JrBVdc-IrkVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_tflite(model_no_quant_tflite, \"test/7/2.png\")"
      ],
      "metadata": {
        "id": "vo7fEutwrnaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_tflite(model_tflite, \"test/7/2.png\")"
      ],
      "metadata": {
        "id": "jzjB_YHprr7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "correct_count = 0\n",
        "wrong_count = 0\n",
        "discarded_count = 0\n",
        "for label_dir in glob.glob(\"test/*\"):\n",
        "  label = int(label_dir.replace(\"test/\", \"\"))\n",
        "  for filename in glob.glob(label_dir + \"/*.png\"):\n",
        "    index, score = predict_tflite(model_tflite, filename)\n",
        "    if score < 0.75:\n",
        "      discarded_count += 1\n",
        "      continue\n",
        "    if index == label:\n",
        "      correct_count += 1\n",
        "    else:\n",
        "      wrong_count += 1\n",
        "      print(\"%d expected, %d found with score %f\" % (label, index, score))\n",
        "      display(Image(filename=filename))\n",
        "\n",
        "correct_percentage = (correct_count / (correct_count + wrong_count)) * 100\n",
        "\n",
        "print(\"%.1f%% correct (N=%d, %d unknown)\" % (correct_percentage, (correct_count + wrong_count), discarded_count))"
      ],
      "metadata": {
        "id": "g_gf9Hahrwwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def get_dir_size(dir):\n",
        "  size = 0\n",
        "  for f in os.scandir(dir):\n",
        "    if f.is_file():\n",
        "      size += f.stat().st_size\n",
        "    elif f.is_dir():\n",
        "      size += get_dir_size(f.path)\n",
        "  return size\n",
        "\n",
        "# Calculate size\n",
        "size_tf = get_dir_size(SAVED_MODEL_FILENAME)\n",
        "size_no_quant_tflite = os.path.getsize(FLOAT_TFL_MODEL_FILENAME)\n",
        "size_tflite = os.path.getsize(QUANTIZED_TFL_MODEL_FILENAME)\n",
        "\n",
        "# Compare size\n",
        "pd.DataFrame.from_records(\n",
        "    [[\"TensorFlow\", f\"{size_tf} bytes\", \"\"],\n",
        "     [\"TensorFlow Lite\", f\"{size_no_quant_tflite} bytes \", f\"(reduced by {size_tf - size_no_quant_tflite} bytes)\"],\n",
        "     [\"TensorFlow Lite Quantized\", f\"{size_tflite} bytes\", f\"(reduced by {size_no_quant_tflite - size_tflite} bytes)\"]],\n",
        "     columns = [\"Model\", \"Size\", \"\"], index=\"Model\")\n"
      ],
      "metadata": {
        "id": "oVmwUPZWrzqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gXbVzcXhvGD"
      },
      "source": [
        "## Create a C source file\n",
        "\n",
        "In the following cell, we convert this model into a C++ source file we can use with TensorFlow Lite for Microcontrollers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8m14_rBPW_5E"
      },
      "source": [
        "# Install xxd if it is not available\n",
        "!apt-get update && apt-get -qq install xxd\n",
        "# Convert to a C source file, i.e, a TensorFlow Lite for Microcontrollers model\n",
        "!xxd -i {QUANTIZED_TFL_MODEL_FILENAME} > {TFL_CC_MODEL_FILENAME}\n",
        "# Update variable names\n",
        "REPLACE_TEXT = QUANTIZED_TFL_MODEL_FILENAME.replace('/', '_').replace('.', '_')\n",
        "!sed -i 's/'{REPLACE_TEXT}'/g_magic_wand_model_data/g' {TFL_CC_MODEL_FILENAME}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEF1HcVIiAfO"
      },
      "source": [
        "# Print the C source file\n",
        "!cat {TFL_CC_MODEL_FILENAME}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LHGV1xVQr-56"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}