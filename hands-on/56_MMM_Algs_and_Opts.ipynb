{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRwp47XUJdAR"
      },
      "source": [
        "# MMM Algorithms and Optimization\n",
        "\n",
        "In this hands-on you will implement and analyze matrix-matrix multiplication (MMM) algorithms. \n",
        "\n",
        "**Goals**:\n",
        "\n",
        "*   Undestand the role of cache in MMM optimization.\n",
        "*   Implement naive, recursive and Strassen algorithms and compare their performance.\n",
        "*   Measure MMM execution times achieved by different frameworks on different hardware.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFbxOSyrv-mB"
      },
      "source": [
        "Let’s refresh our mind about matrix multiplication:\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/1*HCKLKLOm5clIeG-d0cRLhA.gif\" alt=\"MMM\" style=\"width: 150px;\"/>\n",
        "\n",
        "It consists of multiplication and addition, this naive way has cubic complexity. For example, the complexity of the 4x4 matrix multiplication is $O(4^3)$ while 10x10 matrix multiplication is $O(10^3)$.\n",
        "\n",
        "Explore the characteristics of the machine used to run your code. Thanks, Google!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqUCA1uTEIoy"
      },
      "source": [
        "!lscpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZloVXyn_g8Uj"
      },
      "source": [
        "Import several helpful packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tjbpK7-_X3T"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import timeit\n",
        "from tqdm import tqdm\n",
        "from functools import partial\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvXFShmBIGUO"
      },
      "source": [
        "## Row-major and column-major\n",
        "\n",
        "Dense data are stored contiguously in memory, addressed by a single index (the memory address). Array memory ordering schemes translate that single index into multiple indices corresponding to the array coordinates. For example, matrices have two indices: rows and columns. Three-d arrays have three, and so on.\n",
        "\n",
        "*Column-major order* is used by **Fortran, Matlab, R**, and most underlying core linear algebra libraries (BLAS). Sequential address locations are translated into array coordinates $i$, $j$, $k$, … so that the first array coordinates vary most rapidly with address, the next array coordinates less rapidly, and so on. For instance, four address locations $1$, $2$, $3$, $4$ are translated into a two by two matrix coordinate system $(i, j)$ as:\n",
        "\n",
        "```\n",
        "address   i  j\n",
        "  1       1  1\n",
        "  2       2  1\n",
        "  3       1  2\n",
        "  4       2  2\n",
        "```\n",
        "*Row-major* ordering is a different translation between sequential address indices and array coordinates, instead laying sequential data in memory across rows in matrices. Row-major ordering is sometimes called **C/C++** style ordering. For instance address locations 1, 2, 3, 4 are translated into a 2x2 matrix coordinate system (i, j) as:\n",
        "\n",
        "```\n",
        "address   i  j\n",
        "  1       1  1\n",
        "  2       1  2\n",
        "  3       2  1\n",
        "  4       2  2\n",
        "```\n",
        "\n",
        "**Exercise**: How are matrices stored in memory in Python / with NumPy? Row-major or column major?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYAdyKM5JFtI"
      },
      "source": [
        "### START CODE HERE ### (≈ 2 lines of code)\n",
        "pass\n",
        "### END CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Fozf7sVJMTQ"
      },
      "source": [
        "## Cache and Loop Ordering\n",
        "\n",
        "**Exercise**: Implement naive MMM algorithm for computing $A^2$ of matrix $A$ at O(n^3) runtime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_iUcbwBqqNv"
      },
      "source": [
        "# Naive matrix multiplication: b = a * a, a: NxN and b: NxN.\n",
        "# Running time: O(N^3)\n",
        "def naive_mmm(A):\n",
        "  B = np.zeros(A.shape)\n",
        "  ### START CODE HERE ### (≈ 6 lines of code)\n",
        "  pass\n",
        "  ### END CODE HERE ###\n",
        "  return B"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g04FA6-sw0wf"
      },
      "source": [
        "Test your implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "284vAXqtwzCT"
      },
      "source": [
        "a = np.array([[1, 2, 3, 4], [4, 5, 6, 7], [7, 8, 9, 1], [2, 3, 4, 5]])\n",
        "print(naive_mmm(a))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W9YGVlFrIK4"
      },
      "source": [
        "We will use a simple and lightweight cache implementaion `@lru_cache` to keep track of repeatable access to the same elements of a matrix. The cache is fully associative cache with Least Recently Used (LRU) replacement policy.\n",
        "\n",
        "We fix the cache size `maxsize=32`, although you can experiment also with a different cache size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu2sdDi-P7iH"
      },
      "source": [
        "from functools import lru_cache\n",
        "\n",
        "N = 16\n",
        "A = np.random.randn(N,N)\n",
        "B = np.zeros(A.shape)\n",
        "\n",
        "# Fully associative cache. Block size=1. Temporal locality only.\n",
        "@lru_cache(maxsize=32)\n",
        "def access(i,j):\n",
        "  return A[i,j]\n",
        "\n",
        "# Naive matrix multiplication: b = a * a, a: NxN and b: NxN.\n",
        "# Running time: O(N^3)\n",
        "def naive_mmm_cache(n):\n",
        "  ### START CODE HERE ### (≈ 4 lines of code, re-use your previous solution)\n",
        "  pass\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "access.cache_clear()\n",
        "naive_mmm_cache(N)\n",
        "access.cache_info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-C3tOMZ0-Z7"
      },
      "source": [
        "**Exercise**: How does the loop order impact cache performance and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enoqroEmaXPz"
      },
      "source": [
        "Let's measure the number of hits and misses for different matrix sizes: [16, 32, ..., 128]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVL_h4ohMgBN"
      },
      "source": [
        "all_hits = np.array([])\n",
        "all_misses = np.array([])\n",
        "\n",
        "N_max = 128\n",
        "A = np.random.randn(N_max,N_max)\n",
        "\n",
        "Z = range(16, N_max, 1)\n",
        "for n in tqdm(Z):\n",
        "  B = np.zeros(A.shape)\n",
        "  access.cache_clear()    # clear cache\n",
        "  naive_mmm_cache(n)      # compute\n",
        "  x = access.cache_info() # get cache statistics\n",
        "  all_hits = np.append(all_hits, x.hits)\n",
        "  all_misses = np.append(all_misses, x.misses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQV0je1nf6gU"
      },
      "source": [
        "Plot `all_hits` and `all_misses` for multiplying matrices of different sizes. \n",
        "\n",
        "**Exercise**: Re-compute the plot for different orderings of $i$, $j$ and $k$ loops in the matrix computation routine above. Explain the differences!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4rLXmx7die7"
      },
      "source": [
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(Z, all_hits, 'r-')\n",
        "plt.plot(Z, all_misses, 'b-')\n",
        "plt.yscale('log')\n",
        "plt.xlabel('Matrix size')\n",
        "plt.ylabel('Hits / Misses')\n",
        "plt.legend(['Hits', 'Misses'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrDKn0C-lrWO"
      },
      "source": [
        "## Recursive Matrix Multiplication\n",
        "\n",
        "<img src=\"https://www.geeksforgeeks.org/wp-content/uploads/strassen_new.png\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PGQsm5xxmmL"
      },
      "source": [
        "def split(matrix):\n",
        "\t\"\"\"\n",
        "\tSplits a given matrix into quarters.\n",
        "\tInput:  NxN matrix\n",
        "\tOutput: Tuple containing 4 N/2 x N/2 matrices corresponding to a, b, c, d\n",
        "\t\"\"\"\n",
        "  \n",
        "  ### START CODE HERE ### (≈ 3 lines of code)\n",
        "\treturn None\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "def recursive(x, y):\n",
        "\t\"\"\"\n",
        "\tComputes matrix product by divide and conquer approach, recursively.\n",
        "\tInput:  NxN matrices x and y\n",
        "\tOutput: NxN matrix, product of x and y\n",
        "\t\"\"\"\n",
        "\n",
        "\t# Base case when size of matrices is 1x1\n",
        "\tif len(x) == 1:\n",
        "\t\treturn x * y\n",
        "\n",
        "\t# Splitting the matrices into quadrants. This will be done recursively\n",
        "\t# until the base case is reached.\n",
        "  ### START CODE HERE ### (≈ 2 lines of code)\n",
        "\tNone\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "\t# Computing the values of the 4 quadrants of the final matrix c\n",
        "  ### START CODE HERE ### (≈ 4 lines of code)\n",
        "\tc11 = None\n",
        "\tc12 = None\n",
        "\tc21 = None\n",
        "\tc22 = None\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "\t# Combining the 4 quadrants into a single matrix by stacking horizontally and vertically.\n",
        "\tc = np.vstack((np.hstack((c11, c12)), np.hstack((c21, c22))))\n",
        "\treturn c"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZQ4gJYq4_zb"
      },
      "source": [
        "A = np.array([[1, 2, 3, 4], [4, 5, 6, 7], [7, 8, 9, 1], [2, 3, 4, 5]])\n",
        "B = np.zeros(A.shape)\n",
        "print(recursive(A,A))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMatBfchztiE"
      },
      "source": [
        "## Strassen Algorithm\n",
        "\n",
        "<img src=\"https://www.geeksforgeeks.org/wp-content/uploads/stressen_formula_new_new.png\">\n",
        "\n",
        "Hint: Use the same `split` function from before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hS5jyQY1sdB"
      },
      "source": [
        "def strassen(x, y):\n",
        "\t\"\"\"\n",
        "\tComputes matrix product by divide and conquer approach, recursively.\n",
        "\tInput: nxn matrices x and y\n",
        "\tOutput: nxn matrix, product of x and y\n",
        "\t\"\"\"\n",
        "\n",
        "\t# Base case when size of matrices is 1x1\n",
        "\tif len(x) == 1:\n",
        "\t\treturn x * y\n",
        "\n",
        "\t# Splitting the matrices into quadrants. This will be done recursively\n",
        "\t# until the base case is reached.\n",
        "  ### START CODE HERE ### (≈ 2 lines of code)\n",
        "\tNone\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "\t# Computing the 7 products, recursively (p1, p2...p7)\n",
        "  ### START CODE HERE ### (≈ 7 lines of code)\n",
        "\tNone\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "\t# Computing the values of the 4 quadrants of the final matrix c\n",
        "  ### START CODE HERE ### (≈ 4 lines of code)\n",
        "\tc11 = None\n",
        "\tc12 = None\n",
        "\tc21 = None\n",
        "\tc22 = None\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "\t# Combining the 4 quadrants into a single matrix by stacking horizontally and vertically.\n",
        "\tc = np.vstack((np.hstack((c11, c12)), np.hstack((c21, c22))))\n",
        "\treturn c"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-vWogwj6gxg"
      },
      "source": [
        "Test your implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCxI1slN6hJ5"
      },
      "source": [
        "A = np.array([[1, 2, 3, 4], [4, 5, 6, 7], [7, 8, 9, 1], [2, 3, 4, 5]])\n",
        "B = np.zeros(A.shape)\n",
        "print(strassen(A,A))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTP-DpGY7owW"
      },
      "source": [
        "Measure timing with `%timeit` magic command. The number `-n` and repeat `-r` serve different purposes. The *number* controls how many executions are done for each timing and it's used to get representative timings. The *repeat* argument controls how many timings are done and its use is to get accurate statistics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhY5nKt87pJb"
      },
      "source": [
        "N=32\n",
        "A = np.random.randn(N,N)\n",
        "B = np.zeros(A.shape)\n",
        "\n",
        "%timeit -n 10 -r 10 naive_mmm(A)\n",
        "%timeit -n 10 -r 10 recursive(A,A)\n",
        "%timeit -n 10 -r 10 strassen(A,A)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZGvm11Lkak5"
      },
      "source": [
        "## MMM on CPU / GPU with Different Frameworks\n",
        "\n",
        "Change your google colab runtime to using GPU.\n",
        "\n",
        "The NVIDIA System Management Interface (`nvidia-smi` utility) is intended to aid in the management and monitoring of NVIDIA GPU devices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvmYLlTVHiTp"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpHWIxZtKiIq"
      },
      "source": [
        "P0 is the highest performance/power state, P15 is the lowest performance/power state.\n",
        "\n",
        "MiB = Mebibyte\n",
        "GiB = Gibibyte\n",
        "\n",
        "GB vs GiB: One GB is defined as 1000³ (1,000,000,000) bytes and one GiB as 1024³ (1,073,741,824) bytes. That means one GB equals 0.93 GiB."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAwsH2V2IQ8S"
      },
      "source": [
        "Since we changed the runtime, we need to imports all required packages again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Gjyd7qUjyic"
      },
      "source": [
        "import numpy as np \n",
        "import torch\n",
        "\n",
        "import timeit\n",
        "from tqdm import tqdm\n",
        "from functools import partial\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AN0mLN2SLg9T"
      },
      "source": [
        "The functions below implement MMM using optimized primitives provided by different frameworks.\n",
        "\n",
        "NumPy implementation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFU0TP9_-Uf_"
      },
      "source": [
        "def np_mmm(N):\n",
        "  a = np.random.randn(N,N)\n",
        "  b = np.matmul(a,a)\n",
        "  del a, b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZmVZ3Cwv0Nn"
      },
      "source": [
        "TensorFlow implementation. CPU and GPU implementations are provided. \n",
        "\n",
        "By default, tensors are created on the CPU. It is important to note that whenever we want to operate on multiple terms, they need to be on the same device. For instance, if we sum two tensors, we need to make sure that both arguments live on the same device---otherwise the framework would not know where to store the result or even how to decide where to perform the computation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-Q4JoMj-YeC"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def tf_mmm(N):\n",
        "  with tf.device('/cpu:0'):\n",
        "    a = tf.random.normal((N, N))\n",
        "    b = tf.math.multiply(a,a)\n",
        "  del a, b\n",
        "\n",
        "def tf_mmm_gpu(N):\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    a = tf.random.normal((N, N))\n",
        "    b = tf.math.multiply(a,a)\n",
        "  del a, b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyNWXA6kBOKI"
      },
      "source": [
        "PyTorch. Make sure the device is set to GPU. CPU and GPU implementations of MMM with PyTorch are provided."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWkt-gVzM5BL"
      },
      "source": [
        "import torch\n",
        "\n",
        "# setting device on GPU if available, else CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "print()\n",
        "\n",
        "#Additional info when using cuda\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tz2QF-dTbtkf"
      },
      "source": [
        "def torch_mmm(N):\n",
        "  a = torch.randn(N,N)\n",
        "  b = torch.matmul(a,a)\n",
        "  del a, b\n",
        "\n",
        "def torch_mmm_cuda(N):\n",
        "  a = torch.randn(N,N).to(device)\n",
        "  b = torch.matmul(a,a)\n",
        "  del a, b "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8oixayABVQ1"
      },
      "source": [
        "Timing and plotting routines."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WUA1kxR124z"
      },
      "source": [
        "def plot_time(func, inputs, repeats, n_tests):\n",
        "    \"\"\"\n",
        "    Run timer and plot time complexity of `func` using the iterable `inputs`.\n",
        "    Run the function `n_tests` times per `repeats`.\n",
        "    \"\"\"\n",
        "    x, y, yerr = [], [], []\n",
        "    for i in (inputs):\n",
        "        timer = timeit.Timer(partial(func, i))\n",
        "        t = timer.repeat(repeat=repeats, number=n_tests)\n",
        "        x.append(i)\n",
        "        y.append(np.mean(t))\n",
        "        yerr.append(np.std(t) / np.sqrt(len(t)))\n",
        "    plt.errorbar(x, y, yerr=yerr, fmt='-o', label=func.__name__)\n",
        "\n",
        "\n",
        "def plot_times(functions, inputs, repeats=3, n_tests=1, file_name=\"\"):\n",
        "    \"\"\"\n",
        "    Run timer and plot time complexity of all `functions`, using the iterable `inputs`.\n",
        "    Run the functions `n_tests` times per `repeats`.\n",
        "    Adds a legend containing the labels added by `plot_time`.\n",
        "    \"\"\"\n",
        "    for func in tqdm(functions):\n",
        "        plot_time(func, inputs, repeats, n_tests)\n",
        "    plt.legend()\n",
        "    plt.xlabel(\"Input\")\n",
        "    plt.ylabel(\"Time [s]\")\n",
        "    plt.yscale(\"log\")\n",
        "    if not file_name:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.savefig(file_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXteLSYos0VE"
      },
      "source": [
        "The `%time` magic command measures the execution time of the line which follows it using the time python module. `%%time` measures the execution time of the whole cell.\n",
        "\n",
        "*Wall clock time* is the actual amount of time taken to perform a job. This is equivalent to timing your job with a stopwatch and the measured time to complete your task can be affected by anything else that the system happens to be doing at the time.\n",
        "\n",
        "*User time* measures the amount of time the CPU spent running your code. This does not count anything else that might be running, and also does not count CPU time spent in the kernel. \n",
        "\n",
        "*System time* is the operating system CPU time due to system calls from the process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_qBChFF12pM"
      },
      "source": [
        "%%time\n",
        "plot_times([np_mmm, tf_mmm, tf_mmm_gpu, torch_mmm, torch_mmm_cuda], \n",
        "           [1,16,128,256,512,1024,2048,4096,8192], repeats=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtLbOQKcGHTv"
      },
      "source": [
        "**Exercise**: Why is the user time greater than wall clock time? Explain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSTcHERvKmCV"
      },
      "source": [
        "# Links\n",
        "\n",
        "*   [Using Pytorch and Cuda for Large Computation in Google Colabs](https://medium.com/analytics-vidhya/using-pytorch-and-cuda-for-large-computation-in-google-colabs-f1c026c17673)\n",
        "\n"
      ]
    }
  ]
}